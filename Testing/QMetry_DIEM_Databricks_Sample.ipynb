{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e018964-7b7b-4d3d-8d11-2bea6a45e7b7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.removeAll()\n",
    "dbutils.widgets.text(\"Raw_File_Path\",\"/mnt/data/load_date=2023-07-17/101_Daily_Sales.014.20230606062634.csv\")\n",
    "rawFilePath=dbutils.widgets.get(\"Raw_File_Path\")\n",
    "dbutils.widgets.text(\"Gold_File_Path\",\"/mnt/data/gold/101_Daily_Sales.014.20230606062634_gold.csv\")\n",
    "goldFilePath=dbutils.widgets.get(\"Gold_File_Path\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fab3070b-8194-42b9-ad03-6ea8baf6154f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "dfs= spark.read.option(\"header\",\"true\").csv(rawFilePath)\n",
    "dfs.createOrReplaceTempView('vwSrcData')\n",
    "bronzSql=\"select * from vwSrcData where DIV is not null and STORE is not null  and ITEM_CONSUMER_UPC is not null and POS_DATE is not null and POS_NET_DOL_AMOUNT is not null and POS_UOM_QTY is not null and POS_UNITS is not null \" \n",
    "silverDf=spark.sql(bronzSql)\n",
    "i_df = silverDf.filter(\"UPC_TYPE='I'\")\n",
    "c_df =  silverDf.filter(\"UPC_TYPE='C'\").join(i_df,[\"DIV\",\"STORE\",\"ITEM_CONSUMER_UPC\"],\"left_anti\")\n",
    "combined_df = i_df.unionByName(c_df)\n",
    "combined_df.createOrReplaceTempView('vwSilverData')\n",
    "rollupSql=\"SELECT  DIV,STORE, ITEM_CONSUMER_UPC, POS_DATE,SUM(POS_NET_DOL_AMOUNT) AS POS_NET_DOL_AMOUNT, SUM(POS_UOM_QTY) AS POS_UOM_QTY, SUM(POS_UNITS) AS POS_UNITS, FIRST(POS_UOM) AS POS_UOM,ITEM_SRC_DIV, ITEM_SRC_LOC,FIRST(MODALITY) AS MODALITY,FIRST(FULFILLMENT) AS FULFILLMENT,FIRST(UPC_TYPE) AS UPC_TYPE from vwSilverData GROUP BY  DIV,STORE, ITEM_CONSUMER_UPC, POS_DATE, ITEM_SRC_DIV, ITEM_SRC_LOC\"\n",
    "trollup_df = spark.sql(rollupSql)\n",
    "df1=trollup_df.drop(\"FULFILLMENT\",\"MODALITY\",\"UPC_TYPE\")\n",
    "df1.createOrReplaceTempView(\"src_vw\")\n",
    "df2= spark.read.option(\"header\",\"true\").csv(goldFilePath)\n",
    "df2.createOrReplaceTempView(\"gld_vw\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7efdad3-3556-49a7-8481-c2e0689dd5a5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "spark_session = SparkSession.builder.appName('spark_Session').getOrCreate()\n",
    "emp_RDD = spark_session.sparkContext.emptyRDD()\n",
    "columns = StructType([\n",
    "                      StructField('TestCaseSummary',StringType(),False),\n",
    "                      StructField('TestCaseDescription',StringType(),False),\n",
    "                      StructField('Status', StringType(), False),\n",
    "                      StructField('Comments', StringType(), False),\n",
    "                      StructField('Test_Data', StringType(), False)\n",
    "                    ])\n",
    "testResult = spark_session.createDataFrame(data=emp_RDD,schema=columns)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4100b3ae-3551-4e2a-a6bf-c46dcd6db241",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Count Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6ae7786-e408-47c7-bed2-a460dc1cda42",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfsrccnt=spark.sql(\"select count(1) from src_vw minus select count(1) from gld_vw union all select count(1) from gld_vw minus select count(1) from src_vw\")\n",
    "dfsrccountunion=spark.sql(\"select 'src' as tabnm, count(1) from src_vw union all select 'gld' as tabnm, count(1) from gld_vw\")\n",
    "dfrowcount=dfsrccnt.count()\n",
    "testResult=[]\n",
    "if dfrowcount<1:\n",
    "    testResult.append(('Count comparison vs target','Count comparison rolled_up raw file vs ETL Ready file', 'Pass', 'Record count is matching', 'count check'))\n",
    "else:\n",
    "    testResult.append(('Count comparison vs target','Count comparison raw file vs ETL Ready file', 'Fail', 'Record count is not matching, please check dfsrccountunion', 'count check'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c6af1fc-d7d2-4514-bd93-f0afb1a784f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Aggregate Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d391a356-dc5d-4fa4-ba5a-4852889f46fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfaggdiff=spark.sql(\"select  sum(cast(POS_NET_DOL_AMOUNT as decimal(8,3))) , sum(cast(POS_UOM_QTY as decimal(18,4))), sum(POS_UNITS) from src_vw minus select sum(cast(POS_NET_DOL_AMOUNT as decimal(8,3))),sum(cast(POS_UOM_QTY as decimal(18,4))), sum(POS_UNITS) from gld_vw\")\n",
    "dfsaggunion=spark.sql(\"select 'src' as tabnm, sum(cast(POS_NET_DOL_AMOUNT as decimal(8,3))),sum(cast(POS_UOM_QTY as decimal(18,4))), sum(POS_UNITS) from src_vw union all select 'gld' as tabnm, sum(cast(POS_NET_DOL_AMOUNT as decimal(8,3))),sum(cast(POS_UOM_QTY as decimal(18,4))), sum(POS_UNITS) from gld_vw\")\n",
    "dfaggcount=dfaggdiff.count()\n",
    "if dfaggcount<1:\n",
    "    testResult.append(('Aggregate validation','Aggregate validation rolled up raw file vs ETL Ready file', 'Pass', 'Sum of Amount and unit values are matching', 'aggregate validation'))\n",
    "else:\n",
    "    testResult.append(('Aggregate validation', 'Aggregate validation rolled up raw file vs ETL Ready file', 'Fail', 'Sum of Amount and unit values are not matching please refer to dfsaggunion', 'aggregate validation'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e74996fd-6655-4cb4-af65-11ae8b334c5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Minus query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00964bc5-9e69-4c11-a343-7f5848309ca2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfsminust=spark.sql(\"select DIV,STORE,ITEM_CONSUMER_UPC,POS_DATE,cast(POS_NET_DOL_AMOUNT as decimal (8,3)),cast(POS_UOM_QTY as decimal (4,2)),cast(POS_UNITS as decimal) from src_vw minus select lpad(DIV,3,'0'),lpad(STORE,5,'0'),lpad(ITEM_CONSUMER_UPC,13,'0'),POS_DATE,cast(POS_NET_DOL_AMOUNT as decimal (8,3)),cast(POS_UOM_QTY as decimal (4,2)),cast(POS_UNITS as decimal) from gld_vw\")\n",
    "dftminuss=spark.sql(\"select lpad(DIV,3,'0'),lpad(STORE,5,'0'),lpad(ITEM_CONSUMER_UPC,13,'0'),POS_DATE,cast(POS_NET_DOL_AMOUNT as decimal (8,3)),cast(POS_UOM_QTY as decimal (4,2)),cast(POS_UNITS as decimal) from gld_vw minus select DIV,STORE,ITEM_CONSUMER_UPC,POS_DATE,cast(POS_NET_DOL_AMOUNT as decimal (8,3)),cast(POS_UOM_QTY as decimal (4,2)),cast(POS_UNITS as decimal) from src_vw\")\n",
    "dfminusunion=dfsminust.union(dftminuss)\n",
    "dfminuscount=dfminusunion.count()\n",
    "if dfminuscount<1:\n",
    "    testResult.append(('Column level data validation','Minus query rolled-up raw file vs ETL Ready file', 'Pass', 'Data matches raw file vs ETL Ready file','full validation check'))\n",
    "else:\n",
    "    testResult.append(('Column level data validation','Minus query rolled-up raw file vs ETL Ready file', 'Fail', 'Data does not match, please check dfminusunion', 'full validation check'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79de9bbc-bc18-418c-a32e-24f35b06dab6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Duplicate check on KCMS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8896f2e4-1af6-4bcd-a076-678095c3f141",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dfdup=spark.sql(\"select DIV,STORE,ITEM_CONSUMER_UPC,POS_DATE,POS_NET_DOL_AMOUNT,POS_UOM_QTY,POS_UNITS,POS_UOM,ITEM_SRC_DIV,ITEM_SRC_LOC from gld_vw group by DIV,STORE,ITEM_CONSUMER_UPC,POS_DATE,POS_NET_DOL_AMOUNT,POS_UOM_QTY,POS_UNITS,POS_UOM,ITEM_SRC_DIV,ITEM_SRC_LOC having count(1)>2\")\n",
    "dfdupcount=dfdup.count()\n",
    "if dfdupcount<1:\n",
    "    testResult.append(('Duplicate check', 'Check for dupliacte records', 'Pass', 'No duplicate records', 'duplicate check'))\n",
    "else:\n",
    "    testResult.append(('Duplicate check', 'Check for dupliacte records', 'Fail', 'Duplicate records are foundcheck dfdup', 'duplicate check'))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d35fa5df-d7fd-4c25-9964-28019ff16bdd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Result Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d6ed171-e44b-4c47-bc8d-e44fec5fbff4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>TestCaseSummary</th><th>TestCaseDescription</th><th>Status</th><th>Comments</th><th>Test_Data</th></tr></thead><tbody><tr><td>Count comparison vs target</td><td>Count comparison rolled_up raw file vs ETL Ready file</td><td>Pass</td><td>Record count is matching</td><td>count check</td></tr><tr><td>Aggregate validation</td><td>Aggregate validation rolled up raw file vs ETL Ready file</td><td>Pass</td><td>Sum of Amount and unit values are matching</td><td>aggregate validation</td></tr><tr><td>Column level data validation</td><td>Minus query rolled-up raw file vs ETL Ready file</td><td>Pass</td><td>Data matches raw file vs ETL Ready file</td><td>full validation check</td></tr><tr><td>Duplicate check</td><td>Check for dupliacte records</td><td>Pass</td><td>No duplicate records</td><td>duplicate check</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Count comparison vs target",
         "Count comparison rolled_up raw file vs ETL Ready file",
         "Pass",
         "Record count is matching",
         "count check"
        ],
        [
         "Aggregate validation",
         "Aggregate validation rolled up raw file vs ETL Ready file",
         "Pass",
         "Sum of Amount and unit values are matching",
         "aggregate validation"
        ],
        [
         "Column level data validation",
         "Minus query rolled-up raw file vs ETL Ready file",
         "Pass",
         "Data matches raw file vs ETL Ready file",
         "full validation check"
        ],
        [
         "Duplicate check",
         "Check for dupliacte records",
         "Pass",
         "No duplicate records",
         "duplicate check"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "TestCaseSummary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TestCaseDescription",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Comments",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Test_Data",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfResult = spark_session.createDataFrame (data=testResult, schema=columns)\n",
    "dfResult.createOrReplaceTempView(\"testResult_vw\")\n",
    "dfResult.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09fc0d1b-c34e-46c4-a7ab-6aaa5c0aec8a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5790f93d-779a-4f96-ae93-329f8d392aad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>tabnm</th><th>sum(CAST(POS_NET_DOL_AMOUNT AS DECIMAL(8,3)))</th><th>sum(CAST(POS_UOM_QTY AS DECIMAL(18,4)))</th><th>sum(POS_UNITS)</th></tr></thead><tbody><tr><td>src</td><td>20231489.032</td><td>1206146.2691</td><td>5859420.0</td></tr><tr><td>gld</td><td>20231489.032</td><td>1206146.2691</td><td>5859420.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "src",
         "20231489.032",
         "1206146.2691",
         5859420.0
        ],
        [
         "gld",
         "20231489.032",
         "1206146.2691",
         5859420.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "tabnm",
         "type": "\"string\""
        },
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "sum(CAST(POS_NET_DOL_AMOUNT AS DECIMAL(8,3)))",
         "type": "\"decimal(18,3)\""
        },
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "sum(CAST(POS_UOM_QTY AS DECIMAL(18,4)))",
         "type": "\"decimal(28,4)\""
        },
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "sum(POS_UNITS)",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(dfsaggunion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eae0f73d-654a-4fb3-93b4-ba254b1e8943",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>TestCaseSummary</th><th>TestCaseDescription</th><th>Status</th><th>Comments</th><th>Test_Data</th></tr></thead><tbody><tr><td>Count comparison vs target</td><td>Count comparison rolled_up raw file vs ETL Ready file</td><td>Pass</td><td>Record count is matching</td><td>count check</td></tr><tr><td>Aggregate validation</td><td>Aggregate validation rolled up raw file vs ETL Ready file</td><td>Pass</td><td>Sum of Amount and unit values are matching</td><td>aggregate validation</td></tr><tr><td>Column level data validation</td><td>Minus query rolled-up raw file vs ETL Ready file</td><td>Pass</td><td>Data matches raw file vs ETL Ready file</td><td>full validation check</td></tr><tr><td>Duplicate check</td><td>Check for dupliacte records</td><td>Pass</td><td>No duplicate records</td><td>duplicate check</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Count comparison vs target",
         "Count comparison rolled_up raw file vs ETL Ready file",
         "Pass",
         "Record count is matching",
         "count check"
        ],
        [
         "Aggregate validation",
         "Aggregate validation rolled up raw file vs ETL Ready file",
         "Pass",
         "Sum of Amount and unit values are matching",
         "aggregate validation"
        ],
        [
         "Column level data validation",
         "Minus query rolled-up raw file vs ETL Ready file",
         "Pass",
         "Data matches raw file vs ETL Ready file",
         "full validation check"
        ],
        [
         "Duplicate check",
         "Check for dupliacte records",
         "Pass",
         "No duplicate records",
         "duplicate check"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "TestCaseSummary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TestCaseDescription",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Comments",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Test_Data",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "dfResult.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87a9707c-ef73-4a0c-b2a8-474d99ff7e76",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>TestCases</th><th>Summary</th><th>Description</th><th>TestStatus</th><th>Priority</th><th>Assignee</th><th>Test_Data</th><th>Expected_Result</th><th>JIRA</th></tr></thead><tbody><tr><td>TestCase1</td><td>Count comparison vs target</td><td>Counts in both the dataframe must be equal</td><td></td><td>High</td><td>kon7198</td><td>count check</td><td>Count are equal in source and target</td><td>DEART-23755</td></tr><tr><td>TestCase2</td><td>Aggregate validation</td><td>Aggregate validation rolled up raw file vs ETL Ready file</td><td></td><td>High</td><td>kon7198</td><td>aggregate validation</td><td>Sum of Amount and unit values are matching</td><td>DEART-23755</td></tr><tr><td>TestCase3</td><td>Column level data validation</td><td>Column level data validation source vs target</td><td></td><td>High</td><td>kon7198</td><td>full validation check</td><td>Values in source and target dataframe are matching</td><td>DEART-23755</td></tr><tr><td>TestCase4</td><td>Check for dupliacte records</td><td>Duplicates should not be present</td><td></td><td>High</td><td>kon7198</td><td>duplicate check</td><td>No duplicates are present</td><td>DEART-23755</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "TestCase1",
         "Count comparison vs target",
         "Counts in both the dataframe must be equal",
         "",
         "High",
         "kon7198",
         "count check",
         "Count are equal in source and target",
         "DEART-23755"
        ],
        [
         "TestCase2",
         "Aggregate validation",
         "Aggregate validation rolled up raw file vs ETL Ready file",
         "",
         "High",
         "kon7198",
         "aggregate validation",
         "Sum of Amount and unit values are matching",
         "DEART-23755"
        ],
        [
         "TestCase3",
         "Column level data validation",
         "Column level data validation source vs target",
         "",
         "High",
         "kon7198",
         "full validation check",
         "Values in source and target dataframe are matching",
         "DEART-23755"
        ],
        [
         "TestCase4",
         "Check for dupliacte records",
         "Duplicates should not be present",
         "",
         "High",
         "kon7198",
         "duplicate check",
         "No duplicates are present",
         "DEART-23755"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "TestCases",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TestStatus",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Priority",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Assignee",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Test_Data",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Expected_Result",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JIRA",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "testcases_data = [\n",
    "(\"TestCase1\",\"Count comparison vs target\",\"Counts in both the dataframe must be equal\",\"\",\"High\",\"kon7198\",\"count check\",\"Count are equal in source and target\",\"DEART-23755\"),\n",
    "(\"TestCase2\",\"Aggregate validation\",\"Aggregate validation rolled up raw file vs ETL Ready file\",\"\",\"High\",\"kon7198\",\"aggregate validation\",\"Sum of Amount and unit values are matching\",\"DEART-23755\"),\n",
    "(\"TestCase3\",\"Column level data validation\",\"Column level data validation source vs target\",\"\",\"High\",\"kon7198\",\"full validation check\",\"Values in source and target dataframe are matching\",\"DEART-23755\"),\n",
    "(\"TestCase4\",\"Check for dupliacte records\",\"Duplicates should not be present\",\"\",\"High\",\"kon7198\",\"duplicate check\",\"No duplicates are present\",\"DEART-23755\")\n",
    "]\n",
    "\n",
    "testcases_schema = [\"TestCases\", \"Summary\", \"Description\", \"TestStatus\", \"Priority\", \"Assignee\", \"Test_Data\", \"Expected_Result\", \"JIRA\"] \n",
    "\n",
    "df_testcases = spark.createDataFrame(testcases_data, testcases_schema)\n",
    "\n",
    "df_testcases.display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a988e4-2477-447e-a503-7bb16fabd0f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Test_Data</th><th>TestCaseSummary</th><th>TestCaseDescription</th><th>Status</th><th>Comments</th><th>TestCases</th><th>Summary</th><th>Description</th><th>TestStatus</th><th>Priority</th><th>Assignee</th><th>Expected_Result</th><th>JIRA</th></tr></thead><tbody><tr><td>count check</td><td>Count comparison vs target</td><td>Count comparison rolled_up raw file vs ETL Ready file</td><td>Pass</td><td>Record count is matching</td><td>TestCase1</td><td>Count comparison vs target</td><td>Counts in both the dataframe must be equal</td><td></td><td>High</td><td>kon7198</td><td>Count are equal in source and target</td><td>DEART-23755</td></tr><tr><td>aggregate validation</td><td>Aggregate validation</td><td>Aggregate validation rolled up raw file vs ETL Ready file</td><td>Pass</td><td>Sum of Amount and unit values are matching</td><td>TestCase2</td><td>Aggregate validation</td><td>Aggregate validation rolled up raw file vs ETL Ready file</td><td></td><td>High</td><td>kon7198</td><td>Sum of Amount and unit values are matching</td><td>DEART-23755</td></tr><tr><td>full validation check</td><td>Column level data validation</td><td>Minus query rolled-up raw file vs ETL Ready file</td><td>Pass</td><td>Data matches raw file vs ETL Ready file</td><td>TestCase3</td><td>Column level data validation</td><td>Column level data validation source vs target</td><td></td><td>High</td><td>kon7198</td><td>Values in source and target dataframe are matching</td><td>DEART-23755</td></tr><tr><td>duplicate check</td><td>Duplicate check</td><td>Check for dupliacte records</td><td>Pass</td><td>No duplicate records</td><td>TestCase4</td><td>Check for dupliacte records</td><td>Duplicates should not be present</td><td></td><td>High</td><td>kon7198</td><td>No duplicates are present</td><td>DEART-23755</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "count check",
         "Count comparison vs target",
         "Count comparison rolled_up raw file vs ETL Ready file",
         "Pass",
         "Record count is matching",
         "TestCase1",
         "Count comparison vs target",
         "Counts in both the dataframe must be equal",
         "",
         "High",
         "kon7198",
         "Count are equal in source and target",
         "DEART-23755"
        ],
        [
         "aggregate validation",
         "Aggregate validation",
         "Aggregate validation rolled up raw file vs ETL Ready file",
         "Pass",
         "Sum of Amount and unit values are matching",
         "TestCase2",
         "Aggregate validation",
         "Aggregate validation rolled up raw file vs ETL Ready file",
         "",
         "High",
         "kon7198",
         "Sum of Amount and unit values are matching",
         "DEART-23755"
        ],
        [
         "full validation check",
         "Column level data validation",
         "Minus query rolled-up raw file vs ETL Ready file",
         "Pass",
         "Data matches raw file vs ETL Ready file",
         "TestCase3",
         "Column level data validation",
         "Column level data validation source vs target",
         "",
         "High",
         "kon7198",
         "Values in source and target dataframe are matching",
         "DEART-23755"
        ],
        [
         "duplicate check",
         "Duplicate check",
         "Check for dupliacte records",
         "Pass",
         "No duplicate records",
         "TestCase4",
         "Check for dupliacte records",
         "Duplicates should not be present",
         "",
         "High",
         "kon7198",
         "No duplicates are present",
         "DEART-23755"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Test_Data",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TestCaseSummary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TestCaseDescription",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Status",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Comments",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TestCases",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Summary",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Description",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TestStatus",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Priority",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Assignee",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Expected_Result",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JIRA",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "testresults_df = dfResult.join(df_testcases, on = \"Test_Data\", how = \"left\")\n",
    "testresults_df.display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f512f41-3df6-46a5-9888-359ff0972bd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['{\"TestCases\":\"TestCase1\",\"Summary\":\"Count comparison vs target\",\"Description\":\"Counts in both the dataframe must be equal\",\"Status\":\"Pass\",\"Priority\":\"High\",\"Assignee\":\"kon7198\",\"Test_Data\":\"count check\",\"Expected_Result\":\"Count are equal in source and target\",\"JIRA\":\"DEART-23755\"}', '{\"TestCases\":\"TestCase2\",\"Summary\":\"Aggregate validation\",\"Description\":\"Aggregate validation rolled up raw file vs ETL Ready file\",\"Status\":\"Pass\",\"Priority\":\"High\",\"Assignee\":\"kon7198\",\"Test_Data\":\"aggregate validation\",\"Expected_Result\":\"Sum of Amount and unit values are matching\",\"JIRA\":\"DEART-23755\"}', '{\"TestCases\":\"TestCase3\",\"Summary\":\"Column level data validation\",\"Description\":\"Column level data validation source vs target\",\"Status\":\"Pass\",\"Priority\":\"High\",\"Assignee\":\"kon7198\",\"Test_Data\":\"full validation check\",\"Expected_Result\":\"Values in source and target dataframe are matching\",\"JIRA\":\"DEART-23755\"}', '{\"TestCases\":\"TestCase4\",\"Summary\":\"Check for dupliacte records\",\"Description\":\"Duplicates should not be present\",\"Status\":\"Pass\",\"Priority\":\"High\",\"Assignee\":\"kon7198\",\"Test_Data\":\"duplicate check\",\"Expected_Result\":\"No duplicates are present\",\"JIRA\":\"DEART-23755\"}'] \n\n{'TestCase1': {'Summary': 'Count comparison vs target', 'Description': 'Counts in both the dataframe must be equal', 'Status': 'Pass', 'Priority': 'High', 'Assignee': 'kon7198', 'Test_Data': 'count check', 'Expected_Result': 'Count are equal in source and target', 'JIRA': 'DEART-23755'}, 'TestCase2': {'Summary': 'Aggregate validation', 'Description': 'Aggregate validation rolled up raw file vs ETL Ready file', 'Status': 'Pass', 'Priority': 'High', 'Assignee': 'kon7198', 'Test_Data': 'aggregate validation', 'Expected_Result': 'Sum of Amount and unit values are matching', 'JIRA': 'DEART-23755'}, 'TestCase3': {'Summary': 'Column level data validation', 'Description': 'Column level data validation source vs target', 'Status': 'Pass', 'Priority': 'High', 'Assignee': 'kon7198', 'Test_Data': 'full validation check', 'Expected_Result': 'Values in source and target dataframe are matching', 'JIRA': 'DEART-23755'}, 'TestCase4': {'Summary': 'Check for dupliacte records', 'Description': 'Duplicates should not be present', 'Status': 'Pass', 'Priority': 'High', 'Assignee': 'kon7198', 'Test_Data': 'duplicate check', 'Expected_Result': 'No duplicates are present', 'JIRA': 'DEART-23755'}} \n\n{\n    \"TestCase1\": {\n        \"Summary\": \"Count comparison vs target\",\n        \"Description\": \"Counts in both the dataframe must be equal\",\n        \"Status\": \"Pass\",\n        \"Priority\": \"High\",\n        \"Assignee\": \"kon7198\",\n        \"Test_Data\": \"count check\",\n        \"Expected_Result\": \"Count are equal in source and target\",\n        \"JIRA\": \"DEART-23755\"\n    },\n    \"TestCase2\": {\n        \"Summary\": \"Aggregate validation\",\n        \"Description\": \"Aggregate validation rolled up raw file vs ETL Ready file\",\n        \"Status\": \"Pass\",\n        \"Priority\": \"High\",\n        \"Assignee\": \"kon7198\",\n        \"Test_Data\": \"aggregate validation\",\n        \"Expected_Result\": \"Sum of Amount and unit values are matching\",\n        \"JIRA\": \"DEART-23755\"\n    },\n    \"TestCase3\": {\n        \"Summary\": \"Column level data validation\",\n        \"Description\": \"Column level data validation source vs target\",\n        \"Status\": \"Pass\",\n        \"Priority\": \"High\",\n        \"Assignee\": \"kon7198\",\n        \"Test_Data\": \"full validation check\",\n        \"Expected_Result\": \"Values in source and target dataframe are matching\",\n        \"JIRA\": \"DEART-23755\"\n    },\n    \"TestCase4\": {\n        \"Summary\": \"Check for dupliacte records\",\n        \"Description\": \"Duplicates should not be present\",\n        \"Status\": \"Pass\",\n        \"Priority\": \"High\",\n        \"Assignee\": \"kon7198\",\n        \"Test_Data\": \"duplicate check\",\n        \"Expected_Result\": \"No duplicates are present\",\n        \"JIRA\": \"DEART-23755\"\n    }\n}\nCurrent working directory: /databricks/driver\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "import json\n",
    "from pyspark.dbutils import DBUtils\n",
    "\n",
    "testresults_df1 = testresults_df.select(\"TestCases\", \"Summary\", \"Description\", \"Status\", \"Priority\", \"Assignee\",\"Test_Data\", \"Expected_Result\", \"JIRA\")\n",
    "#testresults_df1.display()\n",
    "\n",
    "#Load Test results in Json format:\n",
    "json_data = testresults_df1.toJSON().collect()\n",
    "print(json_data,\"\\n\")\n",
    "\n",
    "testresults_json = {}\n",
    "\n",
    "for json_string in json_data:\n",
    "    json_object = json.loads(json_string)\n",
    "    test_case =json_object['TestCases']\n",
    "    del json_object['TestCases']\n",
    "    testresults_json[test_case] = json_object\n",
    "\n",
    "print(testresults_json,\"\\n\")\n",
    "\n",
    "#Show Test Results in Json format:\n",
    "print(json.dumps(testresults_json, indent=4))\n",
    "\n",
    "#Load Test Results in Json file: to databricks default directory: /databricks/driver\n",
    "#testresults_json_file = testresults_json.to_json('TestResults_QMetry_Integration.json',orient = 'index',indent = 4)\n",
    "\n",
    "with open('/dbfs/Ravi/pyvalidata//pyvalidata_DIEM_TestResults_json_file', 'w') as json_file:\n",
    "    json.dump(testresults_json, json_file, indent=4)\n",
    "\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "#testresults_json_file1.json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06ff7ccf-50d8-496f-9198-f10bbccd0e97",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fle saved at: /dbfs/Ravi/pyvalidata//pyvalidata_DIEM_TestResults_json_file.json\n{\n    \"TestCase1\": {\n        \"Summary\": \"Count comparison vs target\",\n        \"Description\": \"Counts in both the dataframe must be equal\",\n        \"Status\": \"Pass\",\n        \"Priority\": \"High\",\n        \"Assignee\": \"kon7198\",\n        \"Test_Data\": \"count check\",\n        \"Expected_Result\": \"Count are equal in source and target\",\n        \"JIRA\": \"DEART-23755\"\n    },\n    \"TestCase2\": {\n        \"Summary\": \"Aggregate validation\",\n        \"Description\": \"Aggregate validation rolled up raw file vs ETL Ready file\",\n        \"Status\": \"Pass\",\n        \"Priority\": \"High\",\n        \"Assignee\": \"kon7198\",\n        \"Test_Data\": \"aggregate validation\",\n        \"Expected_Result\": \"Sum of Amount and unit values are matching\",\n        \"JIRA\": \"DEART-23755\"\n    },\n    \"TestCase3\": {\n        \"Summary\": \"Column level data validation\",\n        \"Description\": \"Column level data validation source vs target\",\n        \"Status\": \"Pass\",\n        \"Priority\": \"High\",\n        \"Assignee\": \"kon7198\",\n        \"Test_Data\": \"full validation check\",\n        \"Expected_Result\": \"Values in source and target dataframe are matching\",\n        \"JIRA\": \"DEART-23755\"\n    },\n    \"TestCase4\": {\n        \"Summary\": \"Check for dupliacte records\",\n        \"Description\": \"Duplicates should not be present\",\n        \"Status\": \"Pass\",\n        \"Priority\": \"High\",\n        \"Assignee\": \"kon7198\",\n        \"Test_Data\": \"duplicate check\",\n        \"Expected_Result\": \"No duplicates are present\",\n        \"JIRA\": \"DEART-23755\"\n    }\n}\n"
     ]
    }
   ],
   "source": [
    "json_string1 = json.dumps(testresults_json, indent=4)\n",
    "json_sring2 = json.dumps(json_string1, indent=4)\n",
    "\n",
    "with open('/dbfs/Ravi/pyvalidata//pyvalidata_DIEM_TestResults_json_file.json', 'w') as json_file:\n",
    "    json_file.write(json_sring2)\n",
    "\n",
    "#Saving Test Results to json file in databricks directory: to user defined directory:\n",
    "TestResults_Saved_Path = \"/dbfs/Ravi/pyvalidata//pyvalidata_DIEM_TestResults_json_file.json\"\n",
    "print(\"Fle saved at:\", TestResults_Saved_Path)\n",
    "\n",
    "#Reading Test Results form Json file in databricks path:\n",
    "with open(TestResults_Saved_Path, 'r') as json_file:\n",
    "    json_data = json_file.read()\n",
    "#print(json_data) -- Ths giving unformatted data\n",
    "\n",
    "test_cases_dictionary_json = json.loads(json_data)\n",
    "\n",
    "print(test_cases_dictionary_json)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 3937857098700531,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "QMetry_DIEM_Databricks_Sample",
   "widgets": {
    "Gold_File_Path": {
     "currentValue": "/mnt/data/gold/101_Daily_Sales.014.20230606062634_gold.csv",
     "nuid": "492d0abc-a352-4017-9e2c-62bfa92f2cdd",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/data/gold/101_Daily_Sales.014.20230606062634_gold.csv",
      "label": null,
      "name": "Gold_File_Path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "Raw_File_Path": {
     "currentValue": "/mnt/data/load_date=2023-07-17/101_Daily_Sales.014.20230606062634.csv",
     "nuid": "844f1b26-11cf-4e11-a88a-8cba14992dc0",
     "typedWidgetInfo": null,
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "/mnt/data/load_date=2023-07-17/101_Daily_Sales.014.20230606062634.csv",
      "label": null,
      "name": "Raw_File_Path",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
